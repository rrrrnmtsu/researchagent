# Ollama ローカルLLM設定

# Ollama サーバーURL（通常はデフォルトでOK）
OLLAMA_URL=http://localhost:11434

# LLMモデル（推奨: qwen2.5:72b-instruct）
LLM_MODEL=qwen2.5:72b-instruct

# その他のモデルオプション:
# LLM_MODEL=qwen2.5:32b-instruct     # 軽量版（32GB RAM推奨）
# LLM_MODEL=llama3.1:70b-instruct    # Llama 3.1 70B
# LLM_MODEL=llama3.1:8b-instruct     # 超軽量版（16GB RAMでも動作）
# LLM_MODEL=gemma2:27b-instruct      # Google Gemma 2

# LLMプロバイダ
LLM_PROVIDER=ollama

# 並列数（ローカルLLMは遅いので減らす推奨）
CONCURRENCY=2

# 1クエリあたりの最大件数
PER_QUERY_LIMIT=20
